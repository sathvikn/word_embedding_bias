{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook complementing blog post: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for neural nets, benefits over 1 hot encodings\n",
    "\n",
    "Chose GloVE over Word2vec/FastText because of ease of interacting with data, results should be somewhat similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacing with Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we read in the data, and create a list of words contained in the embeddings, as well as a dictionary mapping words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('glove.6B/glove.6B.200d.txt', 'r')\n",
    "vocab = []\n",
    "embeddings = {}\n",
    "matrix = []\n",
    "lemm = WordNetLemmatizer()\n",
    "for line in f:\n",
    "    split_line = line.split()\n",
    "    word = split_line[0]\n",
    "    vector = np.asarray([float(i) for i in split_line[1:]])\n",
    "    embeddings.update({word:vector})\n",
    "    vocab.append(word)\n",
    "    matrix.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the definition from Levy, Goldberg (2014) when evaluating analogies. The canonical example is $queen - king \\approx woman - man$, where $queen$ is the unknown vector.\n",
    "\n",
    "We solve the analogy as follows(where $v$ is a word in the vocabulary, $V$, and $cos$ represents cosine similarity): $$argmax_{v \\in V}(cos(v, woman - man + king)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(word, relation, k = 4, return_list = False):\n",
    "    \"\"\"Takes in a target word, a list of two words expressing a relation,\n",
    "    an integer denoting the amount of values that should be returned, and whether a list of words should be returned,\n",
    "    or only the first item. k is set to 4 by default, in the case that the target word and the relation words\n",
    "    answer the analogy.\n",
    "    Returns a list of words of length [k - 3, k] that satisfy the analogy in order of decreasing cosine similarity\"\"\"\n",
    "    if [i for i in [word, relation[0], relation[1]] if i not in embeddings]:\n",
    "        raise ValueError(\"Word must be in vocabulary\")\n",
    "    result_vector = embeddings[relation[0]] - embeddings[relation[1]] + embeddings[word]\n",
    "    nearest_indices = k_nearest_vectors(k, matrix, [result_vector])[0]\n",
    "    closest_words = [vocab[i] for i in nearest_indices if vocab[i] != word and vocab[i] not in relation]\n",
    "    if return_list:\n",
    "        return closest_words\n",
    "    else:\n",
    "        return closest_words[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the following helper function to reduce the search space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_vectors(k, mtx, candidate_vector):\n",
    "    \"\"\"Takes in an integer value(k), a matrix (2D list) of all the vectors, and the vector (list) we want to compare.\n",
    "    Returns an array of length k for indices of the most similar word vectors, and the cosine similarities of these vectors \"\"\"\n",
    "    cos_similarities = cosine_similarity(mtx, candidate_vector).flatten()\n",
    "    k_sorted = np.flip(np.argsort(cos_similarities)[-k:], axis = 0)\n",
    "    cos_sorted = np.flip(np.sort(cos_similarities), axis = 0)[:k]\n",
    "    return k_sorted, cos_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following test tells us our helper function is correct: the item with the highest cosine similarity to a given word in the dataset will be the word itself. Because the function returns a list of indices, we check if the index corresponds most to the word we are checking by looking at its position in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('woman') == k_nearest_vectors(5, matrix, [embeddings['woman']])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a couple analogies to make sure our evaluation function is working properly. We'll start with the example: \"woman\" is to \"man\" as \"queen\" is to \"king.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'queen'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('king', ['woman', 'man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings can also reveal details like grammatical properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'harder'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('hard', ['better', 'good']) #comparative adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ducks'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('duck', ['men', 'man']) #singular/plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'their'"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('they', ['his', 'he']) #possessive forms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can also help us answer questions about the world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beijing'"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('china', ['moscow', 'russia']) #capitals of countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gda≈Ñsk'"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('danzig', ['mumbai', 'bombay']) #former names of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asia'"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('japan', ['europe', 'germany']) #country-continent mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, embeddings can also reveal problematic *biases* in language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'woman'"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('nurse', ['man', 'doctor']) #gender based on occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'black'"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('criminal', ['white', 'police']) #racial stereotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pakistan'"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate('terrorist', ['india', 'lawful']) #stereotypes of countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to figure out why this is the case by looking at some of the mathematical properties of the embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reduce the dimensionality of our search space, we're only going to consider words that describe people. The WordNet database organizes words based on semantics, so we're going to look at the _hyponyms_(words that could be subcategories of a given word) of the word \"person.\" In WordNet, the entry for a word corresponds to a _synset_, which is a set of _senses_, or contexts, a word could be mentioned in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The senses of \"person\" are listed as follows, we will use the first one: http://wordnetweb.princeton.edu/perl/webwn?s=person&sub=Search+WordNet&o2=&o0=1&o8=1&o1=1&o7=&o5=&o9=&o6=&o3=&o4=&h=\n",
    "\n",
    "Although using built-in tools that perform tasks like named-entity recognition would be useful for this task, these are statistical/neural models that often require the word to be in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyponyms(synset):\n",
    "    \"\"\"Get all words that are considered subcategories, or hyponyms of a particular noun.\n",
    "    Takes in a WordNet Synset object, returns a set of words that are part of that synset\"\"\"\n",
    "    hyponyms = set()\n",
    "    for hyponym in synset.hyponyms():\n",
    "        hyponyms |= set(get_hyponyms(hyponym)) #Gets union of all the hyponyms of the word\n",
    "    hyponyms = hyponyms | set(synset.hyponyms())\n",
    "    return hyponyms\n",
    "\n",
    "types_of_people = {synset.name().split('.')[0] for synset in get_hyponyms(wn.synset('person.n.01'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also hard-code a list of gender-specific words, and add additional logic to a function to tell us if a word is specific to a certain gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From Danielle Sucher's https://github.com/DanielleSucher/Jailbreak-the-Patriarchy\n",
    "male_words=set(['guy','spokesman','chairman',\"men's\",'men','him',\"he's\",'his',\n",
    "                'boy','boyfriend','boyfriends','boys','brother','brothers','dad','dads','dude','father','fathers',\n",
    "                'fiance','gentleman','gentlemen','god','grandfather','grandpa','grandson','groom','he','himself',\n",
    "                'husband','husbands','king','male','man','mr','nephew','nephews','priest','prince','son',\n",
    "                'sons','uncle','uncles','waiter','widower','widowers'])\n",
    "female_words=set(['heroine','spokeswoman','chairwoman',\"women's\",\n",
    "                  'actress','women',\"she's\",'her','aunt','aunts','bride',\n",
    "                  'daughter','daughters','female','fiancee','girl','girlfriend','girlfriends','girls',\n",
    "                  'goddess','granddaughter','grandma','grandmother','herself','ladies','lady','lady','mom',\n",
    "                  'moms','mother','mothers','mrs','ms','niece','nieces','priestess','princess','queens','she',\n",
    "                  'sister','sisters','waitress','widow','widows','wife','wives','woman'])\n",
    "def gender_neutral(word):\n",
    "    return (word not in male_words) and (word not in female_words) and (not word.endswith('man')) and (not word.endswith('woman'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Finding the Closest Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our embeddings for people, let's look at the most similar words to a target word, like \"man.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_persons(k, word, gender_bias = False):\n",
    "    #Here, we can use the cosine similarity array returned by k_nearest_vectors!\n",
    "    k_nearest = k_nearest_vectors(k, matrix, [embeddings[word]])\n",
    "    people_tuples = [(vocab[k_nearest[0][i]], k_nearest[1][i], i) for i in np.arange(len(k_nearest[0])) \n",
    "            if vocab[k_nearest[0][i]] in types_of_people and vocab[k_nearest[0][i]] != word]\n",
    "    if gender_bias:\n",
    "        people_tuples = [t for t in people_tuples if gender_neutral(t[0])]\n",
    "    return people_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('life', 0.6307883963794974, 12),\n",
       " ('soldier', 0.591583998050716, 27),\n",
       " ('victim', 0.5786380246447363, 38),\n",
       " ('friend', 0.5766715687263276, 40),\n",
       " ('hand', 0.5601769983335778, 58),\n",
       " ('suspect', 0.5480306904441015, 85),\n",
       " ('back', 0.5473999376575065, 87),\n",
       " ('hero', 0.5459395422007508, 90),\n",
       " ('face', 0.5428986123388589, 98),\n",
       " ('killer', 0.5320978301464728, 115),\n",
       " ('second', 0.5257842876491682, 130),\n",
       " ('shot', 0.5249789668806233, 132),\n",
       " ('character', 0.5227359871777943, 138),\n",
       " ('great', 0.5185942815595146, 156),\n",
       " ('actor', 0.5092920171540738, 178),\n",
       " ('child', 0.5035236174166932, 203),\n",
       " ('case', 0.502722658076125, 206),\n",
       " ('best', 0.49673880713822366, 235),\n",
       " ('witness', 0.4931514459178098, 247),\n",
       " ('sort', 0.49218978220344056, 251),\n",
       " ('player', 0.49023664128163225, 263),\n",
       " ('attacker', 0.4891015907744395, 266),\n",
       " ('black', 0.48886219780365137, 268),\n",
       " ('lover', 0.4882798709468317, 271),\n",
       " ('officer', 0.48768857661959075, 274),\n",
       " ('head', 0.4851015081745771, 284),\n",
       " ('doctor', 0.48345658170396777, 289),\n",
       " ('murderer', 0.48296941583666486, 292),\n",
       " ('driver', 0.4824417570268411, 294)]"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_persons(300, 'man', gender_bias = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('victim', 0.6407717678973269, 14),\n",
       " ('child', 0.6294301260530981, 16),\n",
       " ('lover', 0.5692697895600373, 28),\n",
       " ('nurse', 0.5646969077183875, 30),\n",
       " ('friend', 0.564160484551198, 31),\n",
       " ('life', 0.5593739976157563, 33),\n",
       " ('soldier', 0.5581014336367704, 36),\n",
       " ('worker', 0.5545941541036009, 39),\n",
       " ('prostitute', 0.5371960748744896, 47),\n",
       " ('baby', 0.5344769899519755, 52),\n",
       " ('teacher', 0.5338314430177431, 53),\n",
       " ('housewife', 0.531849122933435, 54),\n",
       " ('doctor', 0.5293920825671159, 57),\n",
       " ('married', 0.5071280346270718, 82),\n",
       " ('birth', 0.5048731795435029, 87),\n",
       " ('patient', 0.48858733901003293, 118),\n",
       " ('black', 0.4878013038637666, 119),\n",
       " ('attacker', 0.4832373010057014, 140),\n",
       " ('witness', 0.47893709378973437, 153),\n",
       " ('athlete', 0.47582035817977636, 164),\n",
       " ('case', 0.47466957549254385, 169),\n",
       " ('lawyer', 0.4741562391662266, 170),\n",
       " ('stranger', 0.47383268845460397, 173),\n",
       " ('student', 0.4716806493229572, 180),\n",
       " ('politician', 0.47160690569794794, 181),\n",
       " ('journalist', 0.4713477539783594, 184),\n",
       " ('maid', 0.47077156215759614, 187),\n",
       " ('shot', 0.47067472219032136, 188),\n",
       " ('face', 0.47060019307594303, 189),\n",
       " ('servant', 0.4702198434804906, 193),\n",
       " ('blond', 0.47008655118478365, 194),\n",
       " ('colleague', 0.4684097023032704, 199)]"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_persons(200, 'woman', gender_bias = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"man\" and \"woman\", we see that there are a lot of gender-specific words like \"son\" and \"daughter,\" but we also see some trends like \"prostitute\" and \"teacher\" being much closer to the vector for \"woman,\" when we see words like \"soldier\" and \"hero\" being closer to the vector for \"man.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('journalist', 0.5365671755406114, 4),\n",
       " ('resident', 0.5335940780111741, 5),\n",
       " ('american', 0.524323772697212, 7),\n",
       " ('immigrant', 0.5149044179025113, 9),\n",
       " ('canadian', 0.5087608372053418, 10),\n",
       " ('businessman', 0.4919954582587013, 15),\n",
       " ('ordinary', 0.49197911067484723, 16),\n",
       " ('politician', 0.49193664604583437, 17),\n",
       " ('lawyer', 0.4886279012698642, 18),\n",
       " ('advocate', 0.4798776509638012, 21),\n",
       " ('soldier', 0.4706849981014689, 23),\n",
       " ('diplomat', 0.47035736040449666, 24),\n",
       " ('woman', 0.4632373958648325, 29),\n",
       " ('fellow', 0.4516985015782866, 35),\n",
       " ('worker', 0.4481144856053686, 37),\n",
       " ('student', 0.443539956291243, 41),\n",
       " ('member', 0.4352997475956328, 46),\n",
       " ('man', 0.43399099844859074, 50),\n",
       " ('jew', 0.42736058974833313, 58)]"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_persons(60, 'citizen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('migrant', 0.6586581622064355, 2),\n",
       " ('worker', 0.5467345173802629, 7),\n",
       " ('haitian', 0.5171931693127529, 10),\n",
       " ('citizen', 0.5149044179025113, 11),\n",
       " ('refugee', 0.4999778243758676, 19),\n",
       " ('mexican', 0.4850413018421284, 21),\n",
       " ('african-american', 0.4609086396751774, 35),\n",
       " ('child', 0.45092261580120474, 42),\n",
       " ('laborer', 0.44883905259640056, 46),\n",
       " ('jew', 0.43974927338906955, 53),\n",
       " ('native', 0.43409641745666205, 58),\n",
       " ('slave', 0.4277706757138491, 67),\n",
       " ('peasant', 0.42618831955368186, 69),\n",
       " ('mother', 0.41915443702011984, 76),\n",
       " ('settler', 0.41906893504267106, 77),\n",
       " ('ethnic', 0.4190071505342275, 78),\n",
       " ('hmong', 0.41789418111914434, 81),\n",
       " ('homeless', 0.41577220439169393, 87),\n",
       " ('gypsy', 0.4137312893354037, 89),\n",
       " ('cuban', 0.41006546518145714, 94)]"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_persons(100, 'immigrant')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For \"citizen\" and \"immigrant,\" we see terms that are biased toward professional occupations and poverty, respectively. \"Immigrant\" yielded more nationalities than \"citizen.\" Inputting specific nationalities only outputted vectors for other nationalities (i.e. \"american\" is near \"canadian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('muslim', 0.63936552999292, 4),\n",
       " ('religious', 0.6055856303162457, 6),\n",
       " ('prophet', 0.5690866955013705, 10),\n",
       " ('fundamentalist', 0.5615563148585314, 12),\n",
       " ('imam', 0.5159405941281745, 24),\n",
       " ('extremist', 0.5112426507672221, 26),\n",
       " ('sufi', 0.4988681944633398, 33),\n",
       " ('radical', 0.4985477790676014, 34),\n",
       " ('wahhabi', 0.47694639943682626, 53),\n",
       " ('shiite', 0.4716709054408388, 58),\n",
       " ('militant', 0.4659725130662824, 60),\n",
       " ('islamist', 0.46129436122721695, 64),\n",
       " ('cleric', 0.4462623660480373, 72),\n",
       " ('christian', 0.43232337579933255, 87),\n",
       " ('arab', 0.42549909536029623, 97)]"
      ]
     },
     "execution_count": 605,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_persons(100, 'islam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('religious', 0.5704454681564426, 23),\n",
       " ('christian', 0.5650218802997707, 26),\n",
       " ('pagan', 0.5551570324668353, 28),\n",
       " ('roman', 0.48583317469927545, 65),\n",
       " ('catholic', 0.4855725548429508, 66),\n",
       " ('convert', 0.48535247510562557, 67),\n",
       " ('protestant', 0.4639060874298178, 84),\n",
       " ('byzantine', 0.4615717798917954, 86),\n",
       " ('pentecostal', 0.4601624667361376, 90)]"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_persons(100, 'christianity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with religious words, the vector for \"islam\" is also close to words like \"militant\" and \"extremist,\" but this trend does not appear for other religions like Christianity. As a whole, this approach allows us to see some trends, but nothing too specific."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Projections onto Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As detailed in Bolukbasi et al (2016), if we take the dot product of one vector and the vector representing the difference between two words in a relation, we'll get to see how biased that word is in comparison to the rest of the words in our dataset. In linear algebra terms, the vector is being projected onto the space spanned by the biased words. For instance, let's see what happens to our embeddings when compared to $she - he$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def projection(word1, word2, filter_people = False):\n",
    "    dot_products = {}\n",
    "    target_vector = embeddings[word1] - embeddings[word2]\n",
    "    for e in embeddings:\n",
    "        if (filter_people and e in types_of_people) or not filter_people:\n",
    "            dot_products[e] = np.dot(embeddings[e], target_vector)\n",
    "    sorted_by_value = sorted(dot_products.items(), key=lambda kv: kv[1])\n",
    "    return sorted_by_value[:30], sorted_by_value[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('shortstop', -7.866294980667501),\n",
       "  ('quarterback', -7.678623495354001),\n",
       "  ('punter', -7.2444960515638),\n",
       "  ('outfielder', -6.9177984875780005),\n",
       "  ('bowler', -6.8500815299922015),\n",
       "  ('receiver', -6.735542888795701),\n",
       "  ('legate', -6.6034683495648405),\n",
       "  ('linebacker', -6.536085448409601),\n",
       "  ('catcher', -6.4791908237107805),\n",
       "  ('coach', -6.333899579356301),\n",
       "  ('cardinal', -6.297014219020201),\n",
       "  ('whitey', -6.26096189180314),\n",
       "  ('pitcher', -6.252590102720001),\n",
       "  ('tackle', -6.2182935602764005),\n",
       "  ('cornerback', -6.127229501230001),\n",
       "  ('spokesman', -6.098534586542301),\n",
       "  ('halfback', -6.0968848928203),\n",
       "  ('kicker', -6.053579904265401),\n",
       "  ('prebendary', -6.0239528710942),\n",
       "  ('pontifex', -6.0055899539254),\n",
       "  ('general', -5.948584890111202),\n",
       "  ('leader', -5.921962464129061),\n",
       "  ('supremo', -5.874865461852),\n",
       "  ('rusher', -5.85202198737869),\n",
       "  ('lineman', -5.8457150827908),\n",
       "  ('flanker', -5.7509251742503995),\n",
       "  ('playmaker', -5.744284934085),\n",
       "  ('hitter', -5.7391065158421),\n",
       "  ('antipope', -5.668187218470999),\n",
       "  ('cricketer', -5.64110658603222)],\n",
       " [('socialite', 8.135051547006),\n",
       "  ('waitress', 8.191019625925259),\n",
       "  ('spinster', 8.1918603550899),\n",
       "  ('bride', 8.323803133090141),\n",
       "  ('coloratura', 8.33904922576672),\n",
       "  ('feminist', 8.463372036708973),\n",
       "  ('schoolgirl', 8.474791272882598),\n",
       "  ('councilwoman', 8.584112140268),\n",
       "  ('bonesetter', 8.747876168373),\n",
       "  ('heroine', 8.793304448592899),\n",
       "  ('mezzo-soprano', 8.79907368720017),\n",
       "  ('lesbian', 8.817974001672539),\n",
       "  ('assemblywoman', 8.846329919188701),\n",
       "  ('wife', 8.876280363249897),\n",
       "  ('countess', 8.966255818686019),\n",
       "  ('girlfriend', 9.05792865679177),\n",
       "  ('aunt', 9.069382092102398),\n",
       "  ('woman', 9.33278690745772),\n",
       "  ('lady-in-waiting', 9.407345307585201),\n",
       "  ('soprano', 9.43622353668138),\n",
       "  ('daughter', 9.641399044228798),\n",
       "  ('duchess', 9.763334946597102),\n",
       "  ('ballerina', 9.852194711147261),\n",
       "  ('spokeswoman', 10.26893052445988),\n",
       "  ('mother', 10.42850861247624),\n",
       "  ('girl', 10.811953813629799),\n",
       "  ('countrywoman', 10.9034648894945),\n",
       "  ('sister', 11.509474009757199),\n",
       "  ('princess', 11.560644709759238),\n",
       "  ('actress', 14.25239722065388)])"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection('she', 'he', filter_people = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('islamist', -16.6025316387151),\n",
       "  ('shiite', -16.199378103045298),\n",
       "  ('militant', -15.1666539752427),\n",
       "  ('ayatollah', -15.08684344561948),\n",
       "  ('malik', -14.793646995352901),\n",
       "  ('imam', -14.77095608413),\n",
       "  ('muslim', -14.7235459709965),\n",
       "  ('pakistani', -14.5791543856673),\n",
       "  ('khan', -14.3126777920802),\n",
       "  ('cleric', -14.209159374016199),\n",
       "  ('extremist', -14.003685587469),\n",
       "  ('saudi', -13.99863485353275),\n",
       "  ('uzbek', -13.627520026573553),\n",
       "  ('palestinian', -13.391144966903198),\n",
       "  ('terrorist', -13.2252381969951),\n",
       "  ('mullah', -12.8332396852572),\n",
       "  ('sheik', -12.721732502827098),\n",
       "  ('sultan', -12.576778063705),\n",
       "  ('iraqi', -12.352310480974301),\n",
       "  ('arab', -12.274642209769802),\n",
       "  ('mufti', -11.92566948831333),\n",
       "  ('wahhabi', -11.8990821003601),\n",
       "  ('terror', -11.7864842051484),\n",
       "  ('kashmiri', -11.4997480425579),\n",
       "  ('lebanese', -11.160095618294159),\n",
       "  ('fundamentalist', -10.9845281794652),\n",
       "  ('kuwaiti', -10.853081371832799),\n",
       "  ('algerian', -10.77075050081354),\n",
       "  ('jordanian', -10.636387022318699),\n",
       "  ('uighur', -10.589546608259461)],\n",
       " [('olmec', 7.871370855800102),\n",
       "  ('caddo', 7.924439048966099),\n",
       "  ('karelian', 8.240467424534),\n",
       "  ('onondaga', 8.2941385904038),\n",
       "  ('keynesian', 8.304270171164),\n",
       "  ('mississippian', 8.4043609648005),\n",
       "  ('episcopalian', 8.5278982947689),\n",
       "  ('pentecostal', 8.5781299226625),\n",
       "  ('pagan', 8.586869453875089),\n",
       "  ('saxon', 8.618986581943199),\n",
       "  ('curate', 8.6238146375035),\n",
       "  ('catawba', 8.7533281228882),\n",
       "  ('pontifex', 8.783373228533101),\n",
       "  ('merovingian', 8.805865818893),\n",
       "  ('anglican', 8.9037363992483),\n",
       "  ('chela', 8.9586875646992),\n",
       "  ('colonizer', 8.9692464552957),\n",
       "  ('sagittarius', 8.99139135681887),\n",
       "  ('lutheran', 9.0433772850855),\n",
       "  ('presbyterian', 9.17504859419616),\n",
       "  ('methodist', 9.326777648948),\n",
       "  ('ryukyuan', 9.5685724022578),\n",
       "  ('hunter-gatherer', 9.6753313240641),\n",
       "  ('mesoamerican', 9.8056170979462),\n",
       "  ('highlander', 9.859679035584001),\n",
       "  ('unitarian', 9.875168794192401),\n",
       "  ('carolingian', 10.318346798507001),\n",
       "  ('gaul', 10.8301339500069),\n",
       "  ('anglo-saxon', 11.444218029862),\n",
       "  ('palatine', 11.454641435134599)])"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection('christianity', 'islam', filter_people=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('immigrant', -21.82061844267234),\n",
       "  ('migrant', -16.0980524684093),\n",
       "  ('huguenot', -12.8639495360054),\n",
       "  ('gypsy', -12.8619533918221),\n",
       "  ('sephardi', -12.7578259644185),\n",
       "  ('squatter', -12.035755525410181),\n",
       "  ('laborer', -11.8791138126678),\n",
       "  ('refugee', -10.5070822362732),\n",
       "  ('protestant', -10.4690636154425),\n",
       "  ('nativist', -10.13034726979675),\n",
       "  ('bantu', -10.075654766595399),\n",
       "  ('ethnic', -10.0336247598518),\n",
       "  ('brick', -9.950020202637),\n",
       "  ('sharecropper', -9.75597473137455),\n",
       "  ('peasant', -9.6837831575813),\n",
       "  ('slave', -9.4864931854122),\n",
       "  ('menial', -9.2491721379344),\n",
       "  ('settler', -9.2197836398765),\n",
       "  ('ashkenazi', -9.1087288814775),\n",
       "  ('melkite', -9.0962071489494),\n",
       "  ('sicilian', -9.00402892980575),\n",
       "  ('haitian', -8.934244488505),\n",
       "  ('emigrant', -8.8923748715085),\n",
       "  ('bricklayer', -8.8666260641733),\n",
       "  ('homeless', -8.8343280949987),\n",
       "  ('hmong', -8.8019893676988),\n",
       "  ('creole', -8.6676025781352),\n",
       "  ('loader', -8.616801787553179),\n",
       "  ('arawak', -8.43548388444135),\n",
       "  ('mennonite', -8.3981981981005)],\n",
       " [('golfer', 7.63635231673304),\n",
       "  ('mediatrix', 7.706210606338901),\n",
       "  ('corporal', 7.716825179052199),\n",
       "  ('broadcaster', 7.793828267067),\n",
       "  ('endorser', 7.805442160555501),\n",
       "  ('liveryman', 7.8318875458570005),\n",
       "  ('arbiter', 7.836141650729399),\n",
       "  ('mullah', 7.902097269473201),\n",
       "  ('emeritus', 7.912800977583),\n",
       "  ('geophysicist', 7.973124430835701),\n",
       "  ('guarantor', 7.986089152015801),\n",
       "  ('serviceman', 8.037624151362099),\n",
       "  ('columnist', 8.053285804696799),\n",
       "  ('censor', 8.102111252146099),\n",
       "  ('penetrator', 8.141330628152001),\n",
       "  ('balloonist', 8.164111937068402),\n",
       "  ('participant', 8.5056108389471),\n",
       "  ('plenipotentiary', 8.5591174619339),\n",
       "  ('runner-up', 8.5982244411743),\n",
       "  ('cyclist', 8.659281211324249),\n",
       "  ('correspondent', 8.74013061977358),\n",
       "  ('highness', 8.87448390271437),\n",
       "  ('polluter', 8.909619270385601),\n",
       "  ('academician', 8.953458161341501),\n",
       "  ('astronaut', 9.063075899870299),\n",
       "  ('recipient', 9.249338556244101),\n",
       "  ('statesman', 10.39725271953667),\n",
       "  ('ambassador', 10.761984377020452),\n",
       "  ('sovereign', 11.3328926271822),\n",
       "  ('citizen', 15.041510377266091)])"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection('citizen', 'immigrant', filter_people = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works Cited:\n",
    "https://nlp.stanford.edu/projects/glove\n",
    "\n",
    "https://levyomer.files.wordpress.com/2014/04/linguistic-regularities-in-sparse-and-explicit-word-representations-conll-2014.pdf Note: Although this publication comes from an Israeli university, I only acknowledge the individual researchers and not the institution, as it is complicit in the state of Israel's oppressive settler colonial practices.\n",
    "\n",
    "https://arxiv.org/pdf/1607.06520.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Reading:\n",
    "Original Word2Vec paper https://arxiv.org/pdf/1310.4546.pdf\n",
    "\n",
    "Using word embeddings to look at stereotypes in historical texts https://arxiv.org/abs/1711.08412\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience]",
   "language": "python",
   "name": "conda-env-datascience-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
